{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "37332a1e-0334-40e7-b675-34a2d5e9fa46",
    "_uuid": "732c76dce04cac205fcfaa9ae9bd82742f78cab1"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. &nbsp; [Introduction](#1-Introduction)\n",
    "2. &nbsp; [Preamble](#2-Preamble)\n",
    "3. &nbsp; [Data](#3-Data)\n",
    "4. &nbsp; [Visualization](#4-Visualization)\n",
    "5. &nbsp; [Baseline](#5-Baseline:-Naive-Bayes)\n",
    "6. &nbsp; [On Cross Validation](#6-On-Cross-Validation)\n",
    "7. &nbsp; [Models](#7-Models)\n",
    "8. &nbsp; [Conclusion](#8-Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "82185c22-ed1e-4e9e-b70d-6fcd57bcc67e",
    "_uuid": "72f5f83fd4ba764cdade364e18daf684fba336a7"
   },
   "source": [
    "# 1 Introduction\n",
    "\n",
    "This notebook is a simple starter analysis for the Iris dataset.\n",
    "\n",
    "Questions and feedback are welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4355107e-c6be-4e01-9295-fda3aff7a6f8",
    "_uuid": "d7320abe81db4c36201a482e921d529a4aa1e546"
   },
   "source": [
    "### Background\n",
    "\n",
    "Some helpful links about the *Iris* family of flowers:\n",
    "\n",
    "- [Flower Anatomy](http://www.northernontarioflora.ca/flower_term.cfm) from the Northern Ontario Plant Database\n",
    "- [*Iris setosa*](https://www.wildflower.org/plants/result.php?id_plant=IRSE),\n",
    "[*Iris versicolor*](https://www.wildflower.org/plants/result.php?id_plant=IRVE2),\n",
    "and\n",
    "[*Iris virginica*](https://www.wildflower.org/plants/result.php?id_plant=IRVI)\n",
    "from the Lady Bird Johnson Wildflower Center\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "94ee986d-27f2-430f-bda4-e1b90755bff5",
    "_uuid": "07283767c0378d9ae7d4caad650bda9e0fce59e8"
   },
   "source": [
    "### License\n",
    "\n",
    "My work is licensed under CC0:\n",
    "\n",
    "- Overview: https://creativecommons.org/publicdomain/zero/1.0/\n",
    "- Legal code: https://creativecommons.org/publicdomain/zero/1.0/legalcode.txt\n",
    "\n",
    "All other rights remain with their respective owners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c63de0d3-b0b5-427c-8e4c-4adc3a5e5b89",
    "_uuid": "321224d0ea41682fefa6ddbe584e47de0b60100d"
   },
   "source": [
    "# 2 Preamble\n",
    "\n",
    "The usual `python` preamble:\n",
    "\n",
    "- `jupyter` magic\n",
    "- `numpy`\n",
    "- `pandas`\n",
    "- `seaborn` + `matplotlib`\n",
    "- `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d097abae-f08f-46aa-ac19-527d7a763a79",
    "_uuid": "5da359b3ce99361e146274e70b021571662cbf99"
   },
   "source": [
    "Click the `Code` button to take a look at hidden cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "adca9796-f703-4dc9-8ad9-8d19de9b5627",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "f87384eac8c4754993714cc683ec88189b4bed4c"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import make_union\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier,\n",
    "    AdaBoostClassifier, ExtraTreesClassifier)\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (19,5)\n",
    "sns.set(style='whitegrid', color_codes=True, font_scale=1.5)\n",
    "np.set_printoptions(suppress = True, linewidth = 200)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7bbd55c5-c103-40c0-8990-b0fa991b5bb8",
    "_kg_hide-input": true,
    "_uuid": "6b4c1df837dbefef1b17d67d1f9a7f8fca76b41f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "def plot_confmat(\n",
    "        y: pd.Series,\n",
    "        y_hat: Sequence,\n",
    "        rotate_x: int = 0,\n",
    "        rotate_y: int = 'vertical') \\\n",
    "        -> None:\n",
    "    \"\"\"\n",
    "    Plot confusion matrix using `seaborn`.\n",
    "    \"\"\"\n",
    "    classes = y.unique()\n",
    "    ax = sns.heatmap(\n",
    "        confusion_matrix(y, y_hat),\n",
    "        xticklabels=classes,\n",
    "        yticklabels=classes,\n",
    "        annot=True,\n",
    "        square=True,\n",
    "        cmap=\"Blues\",\n",
    "        fmt='d',\n",
    "        cbar=False)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    plt.xticks(rotation=rotate_x)\n",
    "    plt.yticks(rotation=rotate_y, va='center')\n",
    "    plt.xlabel('Predicted Value')\n",
    "    plt.ylabel('True Value')\n",
    "    \n",
    "def seq(start, stop, step=None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Inclusive sequence.\n",
    "    \"\"\"\n",
    "    if step is None:\n",
    "        if start < stop:\n",
    "            step = 1\n",
    "        else:\n",
    "            step = -1\n",
    "\n",
    "    if is_int(start) and is_int(step):\n",
    "        dtype = 'int'\n",
    "    else:\n",
    "        dtype = None\n",
    "\n",
    "    d = max(n_dec(step), n_dec(start))\n",
    "    n_step = np.floor(round(stop - start, d + 1) / step) + 1\n",
    "    delta = np.arange(n_step) * step\n",
    "    return np.round(start + delta, decimals=d).astype(dtype)\n",
    "\n",
    "def is_int(x) -> bool:\n",
    "    \"\"\"\n",
    "    Whether `x` is int.\n",
    "    \"\"\"\n",
    "    return isinstance(x, (int, np.integer))\n",
    "\n",
    "def n_dec(x) -> int:\n",
    "    \"\"\"\n",
    "    Number of decimal places, using `str` conversion.\n",
    "    \"\"\"\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    _, _, dec = str(x).partition('.')\n",
    "    return len(dec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ad358927-f5f2-4724-9088-4171a1b5e2a6",
    "_uuid": "21e185c2930ab89bbae3f6ee07ece88bf6a7a3e5"
   },
   "source": [
    "Let's store the path to our data and set a global seed for reproducible results, then get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "00460d04-78c3-4799-b73f-8cdb1540701f",
    "_uuid": "66beee10e683bd71ce9111cb52399059eaedf416",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv = '../input/Iris.csv'\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e32a7c46-5605-4db1-a743-3bdf07e08d16",
    "_uuid": "c3b2303ec38be5b6c059ac38c4c654a9f6a526b8"
   },
   "source": [
    "# 3 Data\n",
    "\n",
    "We'll load the `csv` using `pandas` and take a look at some summary statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "69b1c3cb-ae74-4272-a1d3-4ca646cb9985",
    "_uuid": "a2b3a24a261464dd6173e5d71d69b45a6d570cfa"
   },
   "source": [
    "## Peek\n",
    "\n",
    "Let's load the first 10 rows of our dataset and make a todo list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": false,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv(csv, nrows=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f4c4820f-9a4e-43af-8d2c-1aac51adc35b",
    "_uuid": "7fbf7a9a6639edf72adecfca6ad089ff7f36e56e"
   },
   "source": [
    "### To do\n",
    "\n",
    "- Drop the `Id` column.\n",
    "- Convert `Species` to `category`.\n",
    "- Split the `DataFrame` into `X` and `y` (predictors and target feature, respectively).\n",
    "- Remove `Iris-` from `Species` labels.  (This is to shrink our confusion matrix.)\n",
    "\n",
    "Note that:\n",
    "\n",
    "- `Species` is our target.\n",
    "- All of our features are floats, which simplifies preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8ebae3e-d66d-40a3-90ac-14f48d2ed3d8",
    "_uuid": "fbb80048afeee367ee7028f41be9845d91ba59b5"
   },
   "source": [
    "## Load Full Dataset\n",
    "\n",
    "Let's load our data (do the todos) and take a look at `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e01e1532-cfa2-4450-8e1b-f796023557cc",
    "_kg_hide-input": true,
    "_uuid": "ea63a56d194d353807576d4647b71d7182cf2ad3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_iris(csv, y='Species'):\n",
    "    df = pd.read_csv(\n",
    "        csv,\n",
    "        usecols=lambda x: x != 'Id',\n",
    "        dtype={y: 'category'},\n",
    "        engine='c',\n",
    "    )\n",
    "    X = df.drop(columns=y)\n",
    "    y = df[y].map(lambda s: s.replace('Iris-', ''))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e289ddf6-1e11-406d-bbe6-60176f97e7f0",
    "_uuid": "3ce670599c982e866dc8d2c50829a4bf8ff08dac",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = load_iris(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "14058caa-b094-45c9-b4ef-298f68e0ab82",
    "_uuid": "a82c5485cf2a519753f74ae125a6c4a22a2e0c0f"
   },
   "source": [
    "## Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2747f48a-6bed-4f41-a3c0-26774a559716",
    "_uuid": "bd1cda5db995619616e819210ab20ac7c7b05fcb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "496d3a49-a8a2-4c38-b8e1-0ebbe90e88a1",
    "_uuid": "9f8dae8337d1890c166ddd43ea82432766d216b4"
   },
   "source": [
    "### Key takeaways\n",
    "\n",
    "- No missing data.\n",
    "- 4 columns.\n",
    "- Our features are 2 dimensional: sepals and petals, so polynomial features (ie,  area) might help.\n",
    "- This is a very small dataset (5 KB), which gives us the luxury of fast model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "03a20929-0e20-493c-bc95-8eb04e0ed23a",
    "_uuid": "319cbbbe88b3916d9b500d3c13860dffd7278dc1"
   },
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c461b950-3194-45cf-ab41-33b0891823a7",
    "_uuid": "ab46e589c5fb7ff133e52f1e9aa99ce151f5fb50",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2989208a-af44-4819-8eae-3c5908748cfd",
    "_uuid": "b944cf7bd8c0f5ca7c4921543550bf29b92478a5"
   },
   "source": [
    "We have a balanced dataset:\n",
    "- 3 species\n",
    "- 50 rows each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6279b29b-fd14-4c87-84f8-6ac4a59135df",
    "_uuid": "e36733ac8a4d03c5fce85eb7e74cd5ed0d726cef"
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Goal: classify flower species\n",
    "- 150 observations\n",
    "- 4 predictors, all floats\n",
    "- 3 balanced target categories\n",
    "- No missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "face4880-c5d8-4437-a68a-6a7fd5a8bb5d",
    "_uuid": "0f30acdcbbfea6b09f871c1783667c174c8eaa89"
   },
   "source": [
    "Next, let's take a closer look at our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b0b8f00d-008e-49ae-9d10-921f7e047e0a",
    "_uuid": "52a5571a0dc5c05926b24c732ccba183f2656653"
   },
   "source": [
    "# 4 Visualization\n",
    "\n",
    "We'll use `seaborn` for 2 plots:\n",
    "\n",
    "- A `pairplot` to see pairwise relationships.\n",
    "- A `boxplot` for feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a123113c-a897-4a6b-bfe4-8df84049f311",
    "_uuid": "736be03eb34cceecaae6b540974aff78e2a41a97"
   },
   "source": [
    "## Pair Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aaf9bdb0-da8f-4603-9ba1-1356bdb4d6b1",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "1dca342dae14f05b4fcf44ff17e4b16505fc9723",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pair(X, y):\n",
    "    sns.pairplot(pd.concat((X, y), axis=1, copy=False), hue=y.name, diag_kind='kde', size=2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f1abff2e-99c4-44cf-ade1-8fed2fabab32",
    "_kg_hide-input": true,
    "_uuid": "b6adac846e05e16abf2c28403fa821f83670e003",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pair(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0facc73e-5fff-4ed5-a5bb-45eec60b3106",
    "_uuid": "1b7917e1dce89a3179e49ae9891fc6690d1fc71e"
   },
   "source": [
    "Pairwise comparisons separate our target quite well, especially *Iris setosa* (blue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bd471772-a0cb-47ff-93a3-362b01d2ec82",
    "_uuid": "c18c03813ec55e87a58983592889c3e9d47e1020"
   },
   "source": [
    "Let's move on to the boxplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "99d2bfd5-adaf-4ac3-a255-ce62a0b735bd",
    "_uuid": "18bccfe14472374bae34dc431e10502f4f85720c"
   },
   "source": [
    "## Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2143a8cf-68fd-4892-99b1-c0bb0e465ecc",
    "_kg_hide-input": false,
    "_uuid": "84ff2f652c17381b282edc3131e97de683ca761a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def box(X):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.boxplot(data=X, orient='v');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "20fbb3d4-b776-4129-9c22-c6c46c416cd3",
    "_kg_hide-input": true,
    "_uuid": "f282d840efd33f3766f10de74444f6d5e14ecef4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "box(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bf946761-eb85-48f7-82c1-a9d399530f91",
    "_uuid": "3fb0e24a120c3925681ed1c2a19b1722facdacbe"
   },
   "source": [
    "This view of our features is useful for feature scaling.  Notably:\n",
    "\n",
    "- All features use the same scale: cm.\n",
    "- Therefore, all features should be strictly positive.\n",
    "- Features occupy different value ranges (eg, sepal length: 4 to 8 cm, petal width: 0 to 3 cm).\n",
    "- And, features have different variances (eg, petal length vs sepal width).\n",
    "\n",
    "Overall, we probably don't need to worry too much about feature scaling for this dataset.  Standardization (*z*-score scaling) should be fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3a5ae257-c5bb-44dd-8b93-06f632dc03f2",
    "_uuid": "1c4322fd83612d1e80b333240e4ffcbd4c0f942d",
    "collapsed": true
   },
   "source": [
    "Next, let's train a simple baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "740b3235-6ecd-4c5d-99f2-9bffdd32f2da",
    "_uuid": "747d967b1a59799cf852e1cae62841abdc10d5ae"
   },
   "source": [
    "# 5 Baseline: Naive Bayes\n",
    "\n",
    "The choice of model is arbitrary, but we want something convenient as our baseline.  So, let's use Gaussian naive Bayes.  It's simple and fast, and it works out of the box with no preprocessing and no tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "35788919-9ce0-43aa-8379-782339b18e2b",
    "_kg_hide-input": false,
    "_uuid": "5174af7c7ef7055ca1c1550f64e820ca01aa4d2c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline(X, y):\n",
    "    acc = GaussianNB().fit(X, y).score(X, y).round(4) * 100\n",
    "    print(f'{acc}% accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2aba03a1-dc3f-4320-9d5d-e534fb4cdba2",
    "_kg_hide-input": true,
    "_uuid": "b5ac11381ae27660dfbc4df28a2dce4b6cfcaeb7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d1a73d17-58f3-4df5-9da3-bc2815ac87a1",
    "_uuid": "6918612f0f57368eafd308da6685059412ffdb6f"
   },
   "source": [
    "Great.  Our baseline is 96% accuracy.  Let's break down our error rate using a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79ab6c3f-6cd1-45d9-bf32-457febeb9df4",
    "_kg_hide-input": false,
    "_uuid": "2b6da9d345fb0ac5e8473dafe2e0096a5da34b4e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confuse(X, y):\n",
    "    plt.figure(figsize=(4.2,4.2))\n",
    "    model = GaussianNB().fit(X, y)\n",
    "    plot_confmat(y, model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "47a970c4-0b3e-4412-9c74-b5821ed835dd",
    "_kg_hide-input": true,
    "_uuid": "b774444f72db172e93f585aea997776c4a7ef833",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confuse(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b0ce6244-9fde-428f-a58d-4e3e6e0216c8",
    "_uuid": "216929822a5b961ff81adde47c484774cb9b0ff2"
   },
   "source": [
    "As expected *Iris setosa* is easy to classify.  On the other hand, we mistake both *versicolor* and *virginica* for the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "56d41c6a-b103-4495-ac7c-3eb17d365971",
    "_uuid": "374821696b8f6a5fdbaaa2502fab7a53fcebb157"
   },
   "source": [
    "Let's explore the performance of our baseline in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "255a30c7-27a3-43d7-b378-772dd754e848",
    "_uuid": "455d624bca96aca7a85cbdcf7a4e6b4f70c46a4d"
   },
   "source": [
    "# 6 On Cross Validation\n",
    "\n",
    "The holy grail of machine learning is generalization.  We want to know how well our model performs on data it hasn't seen before.  Kaggle competitions use a hidden holdout set (aka the *test set*) to uniformly rank submissions, but we don't have that here.  So, let's use cross validation to simulate a holdout set.  The method is simple:\n",
    "\n",
    "1. Split the data into `train` and `test`.\n",
    "2. Fit the model on `train`.\n",
    "3. Measure accuracy on `test`.\n",
    "4. Repeat to taste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8aa7d0f9-9f8e-4e67-bdba-6be5a3e45fee",
    "_uuid": "db974e6158e9d499b22225347dda844da3d7e27a"
   },
   "source": [
    "## How should we split our data?\n",
    "\n",
    "- First, use a fixed seed for reproducible results.\n",
    "- Second, cross validation is often performed using the *k*-fold method, but we'll be using `sklearn`'s `StratifiedShuffleSplit` instead.  This gives us better control over training size, which is important for the next question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a2bd3b7d-ab08-43ea-8040-5e28d6eed316",
    "_uuid": "ba00bf6730d035220a4126328e032dd7f09a406e"
   },
   "source": [
    "## Where should we split our data?\n",
    "\n",
    "This is somewhat arbitrary, so let's try a few different options.\n",
    "\n",
    "- We'll use 10%, 20%, ..., 90% of our total data as `train`, and the remainder will be `test`.\n",
    "- We'll split our data 1000 times for each percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b8233462-78bc-44c4-bb4d-5527597e704c",
    "_uuid": "168864fa62486c5a554a5a00a64761d930ff5586"
   },
   "source": [
    "## To do\n",
    "\n",
    "- Gaussian naive Bayes classifier\n",
    "- 9 percentages, 10% to 90%\n",
    "- 1000 splits for each percentage\n",
    "- 9000 models total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b327a0c2-795c-4d3f-949f-d5253af174a8",
    "_uuid": "265a7f72e4eb446654c39f36540852b8d377405e"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8317c5b2-af74-48bf-92ee-2f7e0b2d5cee",
    "_kg_hide-input": true,
    "_uuid": "c854f8d16cbdbe8ec9f23918cf8c8ee815469c5a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_cv(X, y, model, n_splits, fractions):\n",
    "    history = np.empty((n_splits, len(fractions)))\n",
    "    \n",
    "    for i, fr in enumerate(fractions):\n",
    "        shuffle = StratifiedShuffleSplit(n_splits, train_size=fr, test_size=None, random_state=seed)\n",
    "        for j, (idx_tr, idx_te) in enumerate(shuffle.split(X, y)):\n",
    "            tr_X, tr_y = X.iloc[idx_tr], y.iloc[idx_tr]\n",
    "            te_X, te_y = X.iloc[idx_te], y.iloc[idx_te]\n",
    "            history[j,i] = model.fit(tr_X, tr_y).score(te_X, te_y)\n",
    "    \n",
    "    df = pd.DataFrame(history, columns=[f'{int(fr*150)}' for fr in fractions])\n",
    "    \n",
    "    plt.figure(figsize=(16,7))\n",
    "    sns.boxplot(data=df)\n",
    "    plt.xlabel('Train Size')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.title('Accuracy vs Training Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a31a7044-3abb-4800-aa41-e74fa0748466",
    "_uuid": "6cc8f8c6c674179485d8b9b2bf19e5989aa3b036",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "baseline_cv(X, y, GaussianNB(), n_splits=1000, fractions=seq(0.1, 0.9, step=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6001eb6f-4692-4565-84e8-cf4dd7939a25",
    "_uuid": "3e47c496741116338444b848b1f299b23fc0e411",
    "collapsed": true
   },
   "source": [
    "Key takeaways:\n",
    "\n",
    "- Our baseline model performs well across the board, starting at just 30 observations (20% of our data), with around 95% accuracy.\n",
    "- At `train` size 15, accuracy degrades quite a bit if we get unlucky (down to 65%), but overall, model accuracy is very consistent.\n",
    "\n",
    "### Note\n",
    "\n",
    "As `train` grows, `test` *shrinks*, because the split is complementary, and thus accuracy becomes less granular:\n",
    "\n",
    "- At `train` size 30, each misclassification reduces accuracy by less than 1%.\n",
    "- At `train` size 135, the marginal reduction is over 6%.\n",
    "\n",
    "This is why variance increases left to right, above `train` size 75.  Such a comparison is *not* apples to apples.  The moral of the story is that `test` size matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "51b2a97e-6bef-4930-a272-2ded7344c86c",
    "_uuid": "80dc39378e910a96aae714989846cf4091644cd7"
   },
   "source": [
    "## The Plan\n",
    "\n",
    "Let's use 45 observations as `train` (stratified 15/15/15).  The remaining 105 observations will be `test`.  Hopefully this strikes a good balance between predictive power and generalization, but it's a fairly arbitrary choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "40c01f69-a1db-45b7-97ce-b42ff650b1dc",
    "_uuid": "85e3342db9387fcd26632e431286d920d8fcd275"
   },
   "source": [
    "### Aside\n",
    "\n",
    "We won't be doing any model tuning or stacking in this notebook.  We won't be splitting `train` at all, so our workflow will differ quite a bit from the Kaggle competition setup, which often splits data into at least 4 parts:\n",
    "\n",
    "1. Private leaderboard (final ranking)\n",
    "2. Public leaderboard (rate limited cross validation)\n",
    "3. Train/dev (unlimited cross validation)\n",
    "4. Train/train (true `train` data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "da456ee5-2459-4a2f-9afe-6a8e95a29c04",
    "_uuid": "6740aacf0de814b55148e4fb0c4045e7406df646"
   },
   "source": [
    "# 7 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "56b09a07-06bb-43e8-8a4c-a7f64cda06cb",
    "_uuid": "197fc14bdf4a86fbe8282bdd603493c9b500f3d8"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "For `X`, let's add some polynomial features and use standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "061245d1-37f6-49be-89f3-6bf7501efb5b",
    "_uuid": "3b106cc2024cef50b834c2f84e654886129219a3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_pipeline = make_pipeline(\n",
    "    PolynomialFeatures(interaction_only=True, include_bias=False),\n",
    "    StandardScaler(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "360fda52-dc41-4c2b-834e-5b022cfc1197",
    "_uuid": "af33ef4163102f78ea39add878b2c04e1e3ca773"
   },
   "source": [
    "\n",
    "We'll leave `y` as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0863d915-c028-427b-b2ce-76ea3cb6953a",
    "_uuid": "e4f445f881943ab48a964d7bcb1240f8a5f8e775",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pipeline = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e8f516ce-6f9c-4011-9fee-8060ddca3f74",
    "_kg_hide-input": true,
    "_uuid": "f4313f90aacd450623a19b8e1de9850c9c201bb9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(X, test_X, y, test_y, X_pipeline, y_pipeline, n_train, seed):\n",
    "    if X_pipeline:\n",
    "        X = X_pipeline.fit_transform(X)\n",
    "        test_X = X_pipeline.transform(test_X)\n",
    "\n",
    "    if y_pipeline:\n",
    "        y = y_pipeline.fit_transform(y)\n",
    "        test_y = y_pipeline.transform(y)\n",
    "    \n",
    "    return X, test_X, y, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9c5fe2fb-393d-4ef3-abe7-a29fbb876e96",
    "_uuid": "e194ec29d71e18f99812db756b63904152c3bc3d",
    "collapsed": true
   },
   "source": [
    "## Which Models\n",
    "\n",
    "Here are the models we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a3e1413f-97e9-4a9d-9a9e-25f62f31c347",
    "_uuid": "147027bc95b6203fd35756899f7111e2a54aba17",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=seed),\n",
    "    'NaiveBayes': GaussianNB(),\n",
    "    'kNN': KNeighborsClassifier(),\n",
    "    'ExtraTrees': ExtraTreesClassifier(random_state=seed),\n",
    "    'GradientBoost': GradientBoostingClassifier(random_state=seed),\n",
    "    'Bagg': BaggingClassifier(random_state=seed),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=seed),\n",
    "    'GaussianProc': GaussianProcessClassifier(random_state=seed),\n",
    "    'Logistic': LogisticRegression(random_state=seed),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "321ae328-9359-49d0-8e99-82264512daae",
    "_uuid": "2ffb66e4c472614ded092a09591fdb9ce8537247"
   },
   "source": [
    "Just the default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6d8a8ec2-37b4-4b59-9861-cc21762bb204",
    "_uuid": "c6d8889b20eab06d88ebe9ff1fc50e3b84e6eb83"
   },
   "source": [
    "## Metrics\n",
    "\n",
    "Thus far, we've been using accuracy, but there's no official leaderboard metric.  So, let's also take a look at log loss.\n",
    "\n",
    "### The Plan\n",
    "\n",
    "- `train` size = 45\n",
    "- `test` size = 105 (complement)\n",
    "- 1000 random splits for each model\n",
    "- Use the same seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "de2edb6e-fc22-4a92-9e8e-0b99fcd377cb",
    "_kg_hide-input": true,
    "_uuid": "e5003aef091a9f427c133a25fac756695845f9ff",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv(model_dict, X, y, X_pipeline, y_pipeline, n_train, n_splits, seed):\n",
    "    acc_dict = dict()\n",
    "    loss_dict = dict()\n",
    "    for k, m in model_dict.items():\n",
    "        acc = []\n",
    "        loss = []\n",
    "        shuffle = StratifiedShuffleSplit(n_splits,\n",
    "                                         train_size=n_train,\n",
    "                                         test_size=None,\n",
    "                                         random_state=seed)\n",
    "        for idx_train, idx_test in shuffle.split(X, y):\n",
    "            tr_X, te_X, tr_y, te_y = preprocess(X.iloc[idx_train], X.iloc[idx_test],\n",
    "                                                y.iloc[idx_train], y.iloc[idx_test],\n",
    "                                                X_pipeline, y_pipeline, n_train, seed)\n",
    "            m.fit(tr_X, tr_y)\n",
    "\n",
    "            y_hat_acc = m.predict(te_X)\n",
    "            acc += [accuracy_score(te_y, y_hat_acc)]\n",
    "\n",
    "            y_hat_loss = m.predict_proba(te_X)\n",
    "            loss += [log_loss(te_y, y_hat_loss)]\n",
    "\n",
    "        acc_dict[k] = acc\n",
    "        loss_dict[k] = loss\n",
    "\n",
    "    return pd.DataFrame(acc_dict), pd.DataFrame(loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "986b40b3-3ddc-41ee-b7ae-e8a2dad9becc",
    "_uuid": "9a5079a27cf703e403cca01f0abf2cba7abac066",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "acc, loss = cv(model_dict, X, y, X_pipeline, y_pipeline, n_train=45, n_splits=1000, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1d408c35-2880-4b0c-829a-c790572da5ca",
    "_kg_hide-input": true,
    "_uuid": "a094d752e84047e325ca5e4627f182f1abda7a49",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Accuracy (%): standard deviation')\n",
    "print((np.std(acc).round(4)*100).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "85490af7-70ac-4b2b-9cdc-4baa8269afb3",
    "_kg_hide-input": true,
    "_uuid": "4a83c773120259651aa9a59d999b86d649ed3f4e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Log loss: standard deviation')\n",
    "print((np.std(loss).round(2)).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "76252148-aed8-4308-9c32-1176bb0bba4a",
    "_uuid": "78ea39f34b7b0aabf9d5dcc8bc5f11cbdb3888f8"
   },
   "source": [
    "## Metrics Plots\n",
    "\n",
    "Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a4977783-1306-4c2e-b68c-88501e6a1f2a",
    "_kg_hide-input": true,
    "_uuid": "ed73bea47ddd74a0474afd53c6b4dfd2f8200b07",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics(acc, loss):\n",
    "    plt.figure(figsize=(10,14))\n",
    "    plt.subplots_adjust(hspace=0.6)\n",
    "    \n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title('Accuracy')\n",
    "    sns.boxplot(data=acc*100)\n",
    "    plt.ylim(80,100)\n",
    "    plt.xticks(rotation='vertical')\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.title('Log Loss (lower is better)')\n",
    "    sns.boxplot(data=loss)\n",
    "    plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1f8772a-7eb6-4df0-8e21-be158b4c2091",
    "_uuid": "58fa3f4535269179a553b05f59ec5f91bc99dcab",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_metrics(acc, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a61b698-3e83-40ac-9a44-5c4a1717b2c3",
    "_uuid": "b51453a353efe661175d99ae4b262088a2504588",
    "collapsed": true
   },
   "source": [
    "## Winners and Losers\n",
    "\n",
    "We shouldn't read too much into model performance without having done any optimization, but this is a good starting point.  A few things that might be worth exploring:\n",
    "\n",
    "- Gaussian processes and logistic regression have very stable log loss.\n",
    "- Extra trees performs well.\n",
    "- Accuracy is more or less the same across the board.  Logistic regression stands out, but that might be due to, eg lack of regularization.\n",
    "- Log loss is much more varied, but most models do overlap.  Gaussian processes is a bit high, though.\n",
    "- Both plots have quite a few outliers.\n",
    "\n",
    "Overall, the log loss is difficult to interpret, but 95% accuracy (+/- 2%) seems like a solid baseline for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d7b7e304-41b8-4160-b9ea-d79fb2ae1b15",
    "_uuid": "9c644b314992f77d956199aca1132039aa1476fe",
    "collapsed": true
   },
   "source": [
    "# 8 Conclusion\n",
    "\n",
    "## What's Missing\n",
    "\n",
    "- Train/test split: We used a 45/105 split.  Try different splits--30/120, 60/90, or 90/60--and see what happens.\n",
    "- Error analysis: Take a look at the errors.  Is a particular flower hard to classify?  Does each model misclassify the same flowers?\n",
    "- Algorithm anslysis: Use the bootstrap--or maybe something else--to better understand the bias/variance characteristics of each model.\n",
    "- Optimization: Model parameters, hyperparameters, cross validation, regularization, stacking, etc.\n",
    "- Feature engineering/analysis: Go beyond polynomial features--what works and what doesn't, and why?\n",
    "- Logs: Saving/loading data, and tracking everything we have and haven't tried--a detailed journal of our thought process.\n",
    "- Metrics: What we're optimizing should influence how we optimize.\n",
    "- More models: `xgboost` and `pytorch` or `keras`.\n",
    "\n",
    "## Final Words\n",
    "\n",
    "There are surely mistakes and better ways to go about things.  If you have any ideas, I would appreciate your feedback.\n",
    "\n",
    "That's all for this notebook.  I hope you found something useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b8c2b5cd-bb97-4fec-9cdb-a655dd98eb51",
    "_uuid": "e4a8796a8191fbed076333bfb336c4038e8383b3",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
